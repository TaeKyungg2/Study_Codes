# 대화 요약

## 1. 원-핫 인코딩 (One-Hot Encoding)
- **정의**: 범주형 데이터를 이진 벡터로 변환. 각 범주를 하나의 축(axis)으로 활용.
- **장점**: 모델이 범주 간 순서 관계를 잘못 학습하지 않음.
- **단점**: 범주 수가 많으면 고차원 희소 행렬 발생 → 메모리·연산 비효율.
- **대안**: 레이블 인코딩, 타깃 인코딩, 임베딩, 해싱 트릭, 바이너리 인코딩 등.

## 2. 레이블 인코딩 (Label Encoding)
- 범주를 정수로 매핑(예: 빨강→0, 초록→1, 파랑→2).
- **장점**: 차원 증가 없음, 간단.
- **단점**: 숫자의 크기(순서) 정보가 부여되어 모델이 오해할 수 있음.

## 3. 기타 인코딩 기법
- **타깃 인코딩**: 범주별 타깃 평균값으로 대체.
- **임베딩**: 신경망 내부에서 저차원 벡터로 학습.
- **해싱 트릭**: 해시 함수로 고정 차원 희소 벡터 생성.
- **바이너리 인코딩**: 정수 레이블을 이진수 비트 벡터로 변환.
- **카운트/빈도 인코딩**: 등장 횟수 또는 비율로 대체.

## 4. 트리 계열 모델
- **의사결정 나무**: 이진 분할, 탐욕적 그리디 알고리즘.
- **랜덤 포레스트**: 배깅 + 결정 트리 앙상블.
- **그래디언트 부스팅**: 잔차 보완 방식(XGBoost, LightGBM, CatBoost 등).

## 5. 탐욕적 알고리즘 (Greedy Algorithm)
- 각 노드에서 불순도 감소량(Information Gain, Gini, RSS 등)이 최대인 분할 선택.
- 전역 최적이 아님에도 실용적 근사 해법.

## 6. 가지치기 (Pruning)
- **사전 가지치기**: `max_depth`, `min_samples_leaf`, `min_impurity_decrease` 등.
- **사후 가지치기**: Cost-Complexity Pruning (α-가지치기)로 과적합 제어.

## 7. 앙상블 (Ensemble)
- **배깅(Bagging)**: Random Forest.
- **부스팅(Boosting)**: XGBoost, LightGBM, CatBoost, AdaBoost.
- **스태킹(Stacking)**: 다양한 모델 예측 결과를 메타 모델에 입력.

## 8. 대표적인 머신러닝 모델들

| 입력 데이터 타입      | 문제 유형       | 대표 모델                           | 특징 / 사용 예시                                                     |
|:---------------------|:---------------|:-----------------------------------|:---------------------------------------------------------------------|
| **범주형(카테고리)**    | 분류(Classification) | Naive Bayes (CategoricalNB), Decision Tree / Random Forest, CatBoost / LightGBM, 로지스틱 회귀 | - 단순·빠름, 독립성 가정 필요 (NB) <br> - 범주 처리 강력, 상호작용 포착 (트리) <br> - 높은 성능, 범주형 전용 (부스팅) <br> - 해석 용이, 선형 결정 경계 (로지스틱) |
| **범주형(카테고리)**    | 군집(Clustering)   | K-modes, 계층적 군집(Hierarchical Clustering) | - 카테고리 유사도 기반 (K-modes) <br> - 덴드로그램으로 군집 구조 파악 (계층적) |
| **연속형(수치형)**      | 회귀(Regression)    | 선형 회귀(Linear Regression), 릿지/라쏘 회귀(Ridge/Lasso), SVR, Gaussian Process Regression | - 해석 쉽고 빠름 (선형) <br> - 과적합 방지 (릿지/라쏘) <br> - 비선형, 커널 활용 (SVR/GP) |
| **연속형(수치형)**      | 군집(Clustering)   | K-means, DBSCAN, Gaussian Mixture Model (GMM) | - 빠르고 간단, 구형 가정 (K-means) <br> - 이상치 탐지, 밀도 기반 (DBSCAN) <br> - 확률적 군집 (GMM) |
| **혼합형(범주+수치)**   | 분류·회귀         | RandomForest, XGBoost, LightGBM, CatBoost, MLP, k-NN | - 자동 특성 중요도, 전처리 최소화 (트리·부스팅) <br> - 복잡 비선형 관계 학습 (MLP) <br> - 직관적, 거리척도 중요 (k-NN) |
| **고차원·희소형**      | 분류·회귀         | Naive Bayes, Linear SVM, 로지스틱 회귀 | - 텍스트·추천 시스템에 강함 <br> - 희소 입력 처리에 적합 |
| **시계열·순차형**      | 예측·분류         | ARIMA/SARIMA, Prophet, LSTM/GRU | - 계절성·추세 모델링 (ARIMA) <br> - 복잡 패턴 학습 (RNN 계열) |

## 9. 대표적인 딥러닝 모델들

| 모델 유형                  | 대표 아키텍처               | 주요 용도                         | 특징 / 장·단점                                                                                 |
|:--------------------------:|:----------------------------|:---------------------------------:|:----------------------------------------------------------------------------------------------|
| **다층 퍼셉트론 (MLP)**    | Fully-Connected Network     | 탭형(tabular) 데이터, 간단한 분류·회귀 | • 구조 단순, 과적합 주의, 피처 스케일링 필요                      |
| **합성곱 신경망 (CNN)**   | LeNet, AlexNet, ResNet 등   | 이미지 분류·객체 검출·세그멘테이션  | • 지역 특성 학습, 파라미터 공유, 해석은 어려움            |
| **순환 신경망 (RNN)**     | Vanilla RNN, LSTM, GRU      | 시계열 예측, 자연어 처리(NLP)      | • 시퀀스 처리, 장기 의존성 문제, LSTM/GRU로 완화               |
| **트랜스포머 (Transformer)** | BERT, GPT, ViT 등           | NLP, 시퀀스–시퀀스, 비전          | • Self-Attention, 병렬 학습, 대규모 사전학습                |
| **오토인코더 (Autoencoder)** | Vanilla AE, Variational AE  | 차원 축소, 이상치 탐지, 생성 모델 | • 잠재공간 학습, 노이즈 제거, 생성 품질은 GAN보다 낮음       |
| **생성적 적대 신경망 (GAN)**| DCGAN, StyleGAN, CycleGAN   | 이미지·영상 생성·변환             | • 경쟁 학습, 사실적 생성, 학습 불안정성 존재    |
| **그래프 신경망 (GNN)**    | GCN, GAT, GraphSAGE         | 그래프 데이터(소셜, 분자, 추천)    | • 관계 학습, 메모리·속도 고려 필요       |

## 10. 하이퍼파라미터 튜닝 및 성능 개선
- Grid Search, Random Search, Bayesian Optimization
- 피처 엔지니어링, 앙상블, 정규화, 교차검증, 모델 해석 도구, 모델 경량화 등

## 아래 표에 주요 범주형 인코딩 기법들의 장단점을 정리했습니다.

|              인코딩 기법             |                          장점                         |                     단점                    | 비고                     |
| :-----------------------------: | :-------------------------------------------------: | :---------------------------------------: | :--------------------- |
|    **원-핫 인코딩**<br/>(One-Hot)    |         - 순서 정보 부여 없음<br/>- 대부분 라이브러리 기본 지원         |   - 차원 폭발(𝑘차원) → 메모리·연산 비효율<br/>- 희소 행렬  | 소규모 범주에 유리             |
|     **레이블 인코딩**<br/>(Label)     |             - 1차원 스칼라로 단순<br/>- 차원 증가 없음            |      - 크기(순서) 정보 부여 → 모델이 잘못 해석할 수 있음     | 트리 계열 모델이나 순서형 데이터에 적합 |
|     **타깃 인코딩**<br/>(Target)     |         - 1차원 실수로 차원 효율적<br/>- 통계적 정보(평균) 반영        | - 과적합 위험(스무딩·교차검증 필요)<br/>- 사이킷런 직접 지원 제한 | 회귀/분류 모두 사용 가능         |
|     **임베딩**<br/>(Embedding)     |       - 저차원(𝑑≪𝑘) 밀집 벡터<br/>- 비슷한 범주끼리 근접 학습       |  - 신경망 기반이어야 함<br/>- 초기 설정(𝑑)·학습 안정화 고민  | 딥러닝 모델에 최적             |
|     **해싱 트릭**<br/>(Hashing)     |         - 고정 차원(예: 128) 유지<br/>- 신규 범주 자동 처리        |    - 해시 충돌 가능 → 정보 손실<br/>- 해시 크기 튜닝 필요   | 스트리밍 데이터에 유리           |
|    **바이너리 인코딩**<br/>(Binary)    | - 원-핫 대비 낮은 차원($\lceil\log_2 k\rceil$)<br/>- 희소성 완화 |     - 비트 순서 정보 여전히 존재<br/>- 모델별 해석 주의     | 중간 규모 범주에 적합           |
| **카운트/빈도 인코딩**<br/>(Count/Freq) |            - 1차원 수치로 간단<br/>- 범주 분포 정보 반영           |   - 희소성·샘플링 편향 반영 위험<br/>- 과도한 빈도 차이 문제   | 불균형 데이터 처리 시 유용        |

**팁**

* 작은 범주 수 → **원-핫** or **레이블**
* 순서가 명확한 범주 → **레이블**
* 통계 정보 활용 → **타깃**, **카운트/빈도**
* 딥러닝 모델 사용 시 → **임베딩**
* 초대규모·스트리밍 → **해싱 트릭** or **바이너리**

필요에 따라 위 기법들을 조합하거나, CatBoost·LightGBM 같은 범주형 전용 모델과 함께 사용해 보세요!

